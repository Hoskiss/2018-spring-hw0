{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "# tokenizer = TweetTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 300\n",
    "\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "print(stopword_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cutted_sentences(raw_lines):\n",
    "    sentences = []\n",
    "    for line in raw_lines:\n",
    "        line = line.strip()\n",
    "        line = line.replace(\" ' \", \"'\")\n",
    "        line = re.sub(\"[^a-zA-Z']\", \" \", line)\n",
    "\n",
    "        words = line.lower().split()\n",
    "        words = [word for word in words if word not in stopword_list and len(word)>1]\n",
    "        sentences.append(words)\n",
    "        \n",
    "    print(len(sentences))\n",
    "    return sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_labeled_path = os.path.join(os.getcwd(), \"data\", \"training_nolabel.csv\")\n",
    "total_sentences = None\n",
    "\n",
    "with open(no_labeled_path, 'r') as no_labeled_file:\n",
    "    total_sentences = get_cutted_sentences(no_labeled_file.readlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_path = os.path.join(os.getcwd(), \"saved_model\", 'dimension_300_window_7_skip_gram')\n",
    "word2vec_model = word2vec.Word2Vec.load(word2vec_model_path)\n",
    "\n",
    "print(word2vec_model.wv.vectors.shape)\n",
    "print(len(word2vec_model.wv.vocab))\n",
    "# print(word2vec_model.wv.vocab)\n",
    "\n",
    "print(word2vec_model['bye'])\n",
    "print(word2vec_model.most_similar('fever'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_path = os.path.join(os.getcwd(), \"data\", \"training_label.csv\")\n",
    "labeled_data = []\n",
    "\n",
    "with open(labeled_path, 'r') as labeled_file:\n",
    "    for line in labeled_file.readlines():\n",
    "        (label, text) = line.split(\"+++$+++\")\n",
    "        labeled_data.append([label.strip(), text.strip()])\n",
    "\n",
    "labeled_dataframe = pd.DataFrame(labeled_data, columns =['Label', 'Text']) \n",
    "labeled_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_path = os.path.join(os.getcwd(), \"data\", \"testing_data.csv\")\n",
    "testing_data = []\n",
    "\n",
    "with open(testing_path, 'r') as testing_file:\n",
    "    for line in testing_file.readlines()[1:]:\n",
    "        line_split = line.split(\",\")\n",
    "        testing_id = line_split[0]\n",
    "        text = \",\".join(line_split[1:])\n",
    "        testing_data.append([testing_id.strip(), text.strip()])\n",
    "\n",
    "testing_dataframe = pd.DataFrame(testing_data, columns =['Id', 'Text']) \n",
    "testing_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frame, validation_frame = train_test_split(labeled_dataframe, test_size=0.1, random_state=42)\n",
    "print(training_frame['Label'].value_counts())\n",
    "print(validation_frame['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_x = training_frame['Text'].tolist()\n",
    "training_x = get_cutted_sentences(training_x)\n",
    "print(len(training_x))\n",
    "training_x = [\" \".join(sentence) for sentence in training_x]\n",
    "print(training_x[:20])\n",
    "\n",
    "validation_x = validation_frame['Text'].tolist()\n",
    "validation_x = get_cutted_sentences(validation_x)\n",
    "validation_x = [\" \".join(sentence) for sentence in validation_x]\n",
    "\n",
    "testing_x = testing_dataframe['Text'].tolist()\n",
    "testing_x = get_cutted_sentences(testing_x)\n",
    "testing_x = [\" \".join(sentence) for sentence in testing_x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(x.split()) for x in training_x)\n",
    "print(max_length)\n",
    "\n",
    "max_length = max(len(x.split()) for x in validation_x)\n",
    "print(max_length)\n",
    "\n",
    "max_length = max(len(x.split()) for x in testing_x)\n",
    "print(max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_y = training_frame['Label'].as_matrix()\n",
    "print(training_y.shape)\n",
    "print(training_y[:20])\n",
    "\n",
    "validation_y = validation_frame['Label'].as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from texcla import corpus, data, experiment\n",
    "from texcla.models import TokenModelFactory, AveragingEncoder\n",
    "from texcla.preprocessing import SpacyTokenizer\n",
    "\n",
    "MAX_LEN = 30\n",
    "N_GRAMS = 2\n",
    "EMB_DIMS = 50\n",
    "EPOCHS = 5\n",
    "WORDS_LIMIT = 15000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use the special tokenizer used for constructing the embeddings\n",
    "# tokenizer = SpacyTokenizer()\n",
    "# tokenizer = experiment.setup_data(\n",
    "#     training_x, training_y, tokenizer, 'twitter_train.bin', max_len=MAX_LEN, ngrams=N_GRAMS, limit_top_tokens=WORDS_LIMIT)\n",
    "# experiment.setup_data(validation_x, validation_y, tokenizer, 'twitter_validation.bin', max_len=MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = data.Dataset.load('twitter_train.bin')\n",
    "ds_val = data.Dataset.load('twitter_validation.bin')\n",
    "\n",
    "factory = TokenModelFactory(\n",
    "    ds_train.num_classes, ds_train.tokenizer.token_index, max_tokens=MAX_LEN, embedding_dims=EMB_DIMS, embedding_type=None)\n",
    "word_encoder_model = AveragingEncoder()\n",
    "model = factory.build_model(\n",
    "        token_encoder_model=word_encoder_model, trainable_embeddings=True)\n",
    "# print(ds_val.X)\n",
    "experiment.train(x=ds_train.X, y=ds_train.y, validation_data=(ds_val.X, ds_val.y), model=model,\n",
    "                 word_encoder_model=word_encoder_model, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
