{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/erica/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "# tokenizer = TweetTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "VECTOR_SIZE = 300\n",
    "\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "print(stopword_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cutted_sentences(raw_lines):\n",
    "    sentences = []\n",
    "    for line in raw_lines:\n",
    "        line = line.strip()\n",
    "        line = line.replace(\" ' \", \"'\")\n",
    "        line = re.sub(\"[^a-zA-Z']\", \" \", line)\n",
    "\n",
    "        words = line.lower().split()\n",
    "        words = [word for word in words if word not in stopword_list and len(word)>1]\n",
    "        sentences.append(words)\n",
    "        \n",
    "    print(len(sentences))\n",
    "    return sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178614\n"
     ]
    }
   ],
   "source": [
    "no_labeled_path = os.path.join(os.getcwd(), \"data\", \"training_nolabel.csv\")\n",
    "total_sentences = None\n",
    "\n",
    "with open(no_labeled_path, 'r') as no_labeled_file:\n",
    "    total_sentences = get_cutted_sentences(no_labeled_file.readlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build word2vec\n",
    "# # sg=0 CBOW ; sg=1 skip-gram\n",
    "# model = word2vec.Word2Vec(size=VECTOR_SIZE, min_count=30, window=7, sg=1)\n",
    "# model.build_vocab(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train word2vec model ; shuffle data every epoch\n",
    "# WORD2VEC_TRAINING_TIMES = 20\n",
    "# for _ in range(WORD2VEC_TRAINING_TIMES):\n",
    "#     random.shuffle(sentences)\n",
    "#     model.train(sentences, total_examples=len(sentences), epochs=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_folder = os.path.join(os.getcwd(), \"saved_model\")\n",
    "# if not os.path.exists(saved_folder):\n",
    "#     os.makedirs(saved_folder)\n",
    "\n",
    "# model.save(os.path.join(os.getcwd(), \"saved_model\", 'dimension_300_window_7_skip_gram'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erica/anaconda3/envs/tf/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03549091  0.08357497  0.02273213  0.2484157   0.06983647 -0.05439604\n",
      " -0.07452954 -0.28196386 -0.08864312 -0.46760035 -0.03336187  0.04060542\n",
      " -0.19226886 -0.17290267  0.31306562 -0.17559303  0.01503302 -0.04937536\n",
      "  0.18731526 -0.13611847  0.1181233  -0.1360042   0.2720578  -0.82264704\n",
      "  0.0115023  -0.2562093   0.2668844   0.05964758 -0.34070194 -0.1497924\n",
      "  0.3708003   0.02961739 -0.09593108 -0.0475378  -0.10257836 -0.2504861\n",
      "  0.13461295 -0.01126851 -0.1218561   0.07747226 -0.2618634   0.22613083\n",
      "  0.08969725  0.12427152 -0.37542117  0.2501521  -0.0499243   0.12466338\n",
      " -0.01164663 -0.39300147 -0.05939501 -0.13669845 -0.05366972  0.16360885\n",
      " -0.3533009  -0.1434421   0.06425     0.02739011 -0.1084506   0.24151093\n",
      " -0.34176365 -0.00194857 -0.03965796  0.1459657   0.04448299 -0.14057057\n",
      "  0.09534362 -0.21862091 -0.1830834  -0.21467595  0.1029019   0.06562733\n",
      " -0.39670658  0.24311852  0.17941189 -0.13037372  0.5111816   0.06188105\n",
      " -0.01294602 -0.26524657 -0.18183404 -0.26294202  0.05998885  0.01909815\n",
      " -0.55496645  0.1611774   0.0314317   0.01565767 -0.0403778   0.01600735\n",
      "  0.00699163  0.01467465 -0.19372728  0.2173296   0.01686825 -0.22321534\n",
      " -0.09600987  0.5610375   0.03098541 -0.48527363 -0.12613562  0.22692959\n",
      " -0.00735561 -0.20298412 -0.13334168 -0.12067669  0.265366   -0.0650266\n",
      " -0.26529488  0.68250656  0.61158335 -0.25649592  0.2970163  -0.12939966\n",
      " -0.07618893 -0.15705459  0.37138012 -0.08243378  0.2611089   0.09286932\n",
      "  0.36339265  0.21522853 -0.12317245  0.15144305  0.2990331   0.02775282\n",
      " -0.2477835   0.14469947  0.58450174 -0.28309184  0.18299468 -0.20010836\n",
      "  0.08494978  0.0466884  -0.1776111  -0.21246268 -0.2776321   0.08033051\n",
      "  0.1516322  -0.03044846  0.20910689  0.17663053  0.07563955  0.40308315\n",
      "  0.05755299 -0.4842408  -0.04262097 -0.11527057 -0.12442984 -0.2749606\n",
      " -0.279201   -0.47000897  0.37708595 -0.577566    0.11332224  0.41151544\n",
      "  0.5598133   0.10560299 -0.08736012 -0.40323526  0.20008229 -0.0748008\n",
      " -0.16662794 -0.06880407  0.25848106  0.2680563   0.27719352 -0.06160948\n",
      "  0.16557923 -0.40744507  0.08131399 -0.07506585  0.35034466 -0.21172059\n",
      "  0.10462712 -0.00627675  0.03638788  0.41643307  0.05762406  0.46872932\n",
      " -0.17707106 -0.1175568  -0.1689797   0.3597017   0.07882068 -0.2604931\n",
      " -0.03071327  0.00575828 -0.04961305  0.18340656 -0.35259444 -0.05029328\n",
      " -0.37224635  0.01547502  0.16071716 -0.32447392 -0.32035375  0.15512416\n",
      " -0.03498797 -0.04915732  0.38730034 -0.11551423 -0.2959563   0.23734474\n",
      " -0.3588302   0.15998663 -0.08969725  0.14059658 -0.32522005  0.08416736\n",
      " -0.60298604 -0.13094832  0.09531606 -0.37768152  0.02409903 -0.08791116\n",
      " -0.03567859 -0.13134627 -0.14881004  0.2150683  -0.05138794 -0.02145649\n",
      " -0.2207088  -0.06183501  0.27398026 -0.02910377 -0.1801686  -0.02745972\n",
      "  0.03503362  0.19352543  0.43743166  0.10463127  0.04642295  0.4322912\n",
      "  0.0632657   0.05503159  0.1087488   0.10800964  0.42629433  0.03440119\n",
      "  0.26475143  0.06800992  0.04808295  0.09769154 -0.27406877 -0.18566836\n",
      " -0.12151503  0.2961903  -0.09542951  0.00237547  0.02794348  0.27978182\n",
      "  0.12978497 -0.5247478  -0.12790187  0.10353323 -0.22157581  0.1472576\n",
      " -0.17864634  0.12761904  0.5095068   0.27699396 -0.15350613 -0.19070996\n",
      "  0.3982913  -0.46454412 -0.08507194  0.14749672 -0.3097379   0.04221852\n",
      "  0.4147525  -0.22132064 -0.21448168  0.01573729 -0.3651784  -0.19419716\n",
      "  0.42542037  0.15504569  0.08975543 -0.2469472  -0.09476275  0.4332562\n",
      " -0.24920335 -0.1418747  -0.12309242  0.26344347  0.0718958  -0.2440525\n",
      "  0.06677384  0.31553003  0.08448908 -0.3177194  -0.2949735   0.0689391\n",
      " -0.35341558  0.03011781  0.04634251 -0.40977058  0.19790989  0.08818088]\n",
      "[('sick', 0.4835624694824219), ('throat', 0.4668008089065552), ('coughing', 0.46350526809692383), ('bronchitis', 0.4629276990890503), ('strep', 0.4581431746482849), ('cough', 0.44587358832359314), ('flu', 0.4423021674156189), ('congestion', 0.4411759674549103), ('sinus', 0.43661588430404663), ('runny', 0.4364381432533264)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erica/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/home/erica/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "word2vec_model_path = os.path.join(os.getcwd(), \"saved_model\", 'dimension_300_window_7_skip_gram')\n",
    "word2vec_model = word2vec.Word2Vec.load(word2vec_model_path)\n",
    "\n",
    "print(word2vec_model['bye'])\n",
    "print(word2vec_model.most_similar('fever'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>are wtf ... awww thanks !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>leavingg to wait for kaysie to arrive myspacin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i wish i could go and see duffy when she comes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i know eep ! i can ' t wait for one more day ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>so scared and feeling sick . fuck ! hope someo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text\n",
       "0     1                          are wtf ... awww thanks !\n",
       "1     1  leavingg to wait for kaysie to arrive myspacin...\n",
       "2     0  i wish i could go and see duffy when she comes...\n",
       "3     1  i know eep ! i can ' t wait for one more day ....\n",
       "4     0  so scared and feeling sick . fuck ! hope someo..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_path = os.path.join(os.getcwd(), \"data\", \"training_label.csv\")\n",
    "labeled_data = []\n",
    "\n",
    "with open(labeled_path, 'r') as labeled_file:\n",
    "    for line in labeled_file.readlines():\n",
    "        (label, text) = line.split(\"+++$+++\")\n",
    "        labeled_data.append([label.strip(), text.strip()])\n",
    "\n",
    "labeled_dataframe = pd.DataFrame(labeled_data, columns =['Label', 'Text']) \n",
    "labeled_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my dog ate our dinner . no , seriously ... he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>omg last day sooon n of primary noooooo x im g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>stupid boys .. they ' re so .. stupid !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>hi ! do u know if the nurburgring is open for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>having lunch in the office , and thinking of h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Id                                               Text\n",
       "0  0  my dog ate our dinner . no , seriously ... he ...\n",
       "1  1  omg last day sooon n of primary noooooo x im g...\n",
       "2  2            stupid boys .. they ' re so .. stupid !\n",
       "3  3  hi ! do u know if the nurburgring is open for ...\n",
       "4  4  having lunch in the office , and thinking of h..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_path = os.path.join(os.getcwd(), \"data\", \"testing_data.csv\")\n",
    "testing_data = []\n",
    "\n",
    "with open(testing_path, 'r') as testing_file:\n",
    "    for line in testing_file.readlines()[1:]:\n",
    "        line_split = line.split(\",\")\n",
    "        testing_id = line_split[0]\n",
    "        text = \",\".join(line_split[1:])\n",
    "        testing_data.append([testing_id.strip(), text.strip()])\n",
    "\n",
    "testing_dataframe = pd.DataFrame(testing_data, columns =['Id', 'Text']) \n",
    "testing_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    90014\n",
      "0    89986\n",
      "Name: Label, dtype: int64\n",
      "1    10031\n",
      "0     9969\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "training_frame, validation_frame = train_test_split(labeled_dataframe, test_size=0.1, random_state=42)\n",
    "print(training_frame['Label'].value_counts())\n",
    "print(validation_frame['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n",
      "180000\n",
      "[['lost', 'aww'], [\"i've\", 'reading', 'news', 'seems', 'keep', 'getting', 'worse', 'terrible'], [\"i'm\", 'seriously', 'upset', 'sad', 'ipod'], ['headed', 'dha', 'school', 'see', 'dance', 'party', 'think', 'dhats', 'paqe', 'lol'], ['biceps', 'killing', \"can't\", 'go', 'gym', 'today'], ['sounds', 'good', 'close'], ['messing', 'myspace', 'twitter', 'yahoo', 'messenger', 'hanging', 'beka', 'go', 'dairy', 'queen'], ['people', 'follow'], ['oh', 'yes', 'much', 'consumption', 'ice', 'cream', 'feel', 'bit', 'sick', 'though'], ['thanks', 'jacey', 'omg', 'boone', 'died', 'wtf', 'expecting'], ['fun'], ['wish', 'even', 'gm', 'though', 'babe'], ['rearranged', 'room', 'looks', 'spacious'], ['funny', 'say', 'past', 'times', \"i've\", 'met', \"i've\", 'huge', 'pimple', 'chin', 'well', 'scars'], ['hospital', 'feel', 'bad', 'jeremys', 'car', 'really', 'hope', 'hes', 'ok'], ['acho', 'lidooooo'], ['munchin', 'yummii', 'white', 'choco', 'cookies', 'lushh', 'nandos', 'hot', 'extra', 'chicken', 'oh', 'dearrrrr', 'rainn', 'thunderrr'], ['hmm', 'thanks', 'must', 'bummer'], ['raining', 'town'], ['spelling', 'entirely', 'correct', 'wants', 'american', 'speller']]\n",
      "20000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "training_x = training_frame['Text'].tolist()\n",
    "training_x = get_cutted_sentences(training_x)\n",
    "print(len(training_x))\n",
    "print(training_x[:20])\n",
    "\n",
    "validation_x = validation_frame['Text'].tolist()\n",
    "validation_x = get_cutted_sentences(validation_x)\n",
    "\n",
    "testing_x = testing_dataframe['Text'].tolist()\n",
    "testing_x = get_cutted_sentences(testing_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "22\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "max_length = max(len(x) for x in training_x)\n",
    "print(max_length)\n",
    "\n",
    "max_length = max(len(x) for x in validation_x)\n",
    "print(max_length)\n",
    "\n",
    "max_length = max(len(x) for x in testing_x)\n",
    "print(max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000,)\n",
      "['0' '0' '0' '1' '0' '0' '1' '1' '0' '0' '0' '0' '1' '0' '0' '1' '0' '0'\n",
      " '0' '0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erica/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/erica/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "training_y = training_frame['Label'].as_matrix()\n",
    "print(training_y.shape)\n",
    "print(training_y[:20])\n",
    "\n",
    "validation_y = validation_frame['Label'].as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_model.wv.vocab.keys(), 所有可以轉成vector的字, length: 14386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "# vectorizer = TfidfVectorizer(analyzer='word', min_df=10)\n",
    "# matrix = vectorizer.fit_transform([word for sentence in total_sentences for word in sentence])\n",
    "# tfidf_map = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "# print ('vocab size :', len(tfidf_map))\n",
    "\n",
    "#Save the tfidf \n",
    "tfidf_map_name = \"tfidf_map.pickle\"\n",
    "# with open(tfidf_map_name, \"wb\") as pickle_file:\n",
    "#     pickle.dump(tfidf_map, pickle_file)\n",
    "with open(tfidf_map_name, \"rb\") as pickle_file:\n",
    "    tfidf_map = pickle.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vector(tokens, vector_size=300, token_size=30):\n",
    "    vector = np.zeros((token_size, vector_size))\n",
    "    index = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            # vector[index] = (word2vec_model[word].reshape((1, vector_size))) * tfidf_map[word] # combining w2v vectors with tfidf value of words in the tweet.\n",
    "            vector[index] = (word2vec_model[word].reshape((1, vector_size))) \n",
    "            index += 1\n",
    "        except KeyError: # handling the case where the token is not\n",
    "#             print(word)\n",
    "#             print(word2vec_model[word])\n",
    "#             print(tfidf_map[word])\n",
    "            \n",
    "            continue\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erica/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000, 30, 300)\n",
      "[[ 0.18590826 -0.02307397  0.30161124 ... -0.0933855  -0.24173458\n",
      "   0.18248731]\n",
      " [ 0.09081154  0.02723532  0.13087037 ... -0.12416029 -0.17585213\n",
      "   0.01903093]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "(20000, 30, 300)\n",
      "(200000, 30, 300)\n"
     ]
    }
   ],
   "source": [
    "training_vector_x = np.array([get_document_vector(documnet) for documnet in training_x])\n",
    "print(training_vector_x.shape)\n",
    "print(training_vector_x[0])\n",
    "\n",
    "validation_vector_x = np.array([get_document_vector(documnet) for documnet in validation_x])\n",
    "print(validation_vector_x.shape)\n",
    "\n",
    "testing_vector_x = np.array([get_document_vector(documnet) for documnet in testing_x])\n",
    "print(testing_vector_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/erica/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/erica/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 64)            93440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 48)            21696     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 48)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 48)            18624     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 48)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 48)            18624     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 48)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 16)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 157,697\n",
      "Trainable params: 157,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/erica/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/200\n",
      "180000/180000 [==============================] - 818s 5ms/step - loss: 0.5103 - acc: 0.7514 - val_loss: 0.4780 - val_acc: 0.7683\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76830, saving model to /home/erica/sean_go/2018-spring-hw0/multiple_lstm_without_tfidf_classifier.h5\n",
      "Epoch 2/200\n",
      "180000/180000 [==============================] - 773s 4ms/step - loss: 0.4784 - acc: 0.7730 - val_loss: 0.4635 - val_acc: 0.7836\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76830 to 0.78360, saving model to /home/erica/sean_go/2018-spring-hw0/multiple_lstm_without_tfidf_classifier.h5\n",
      "Epoch 3/200\n",
      "180000/180000 [==============================] - 774s 4ms/step - loss: 0.4651 - acc: 0.7807 - val_loss: 0.4609 - val_acc: 0.7799\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.78360\n",
      "Epoch 4/200\n",
      "180000/180000 [==============================] - 776s 4ms/step - loss: 0.4538 - acc: 0.7875 - val_loss: 0.4576 - val_acc: 0.7857\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.78360 to 0.78575, saving model to /home/erica/sean_go/2018-spring-hw0/multiple_lstm_without_tfidf_classifier.h5\n",
      "Epoch 5/200\n",
      "180000/180000 [==============================] - 794s 4ms/step - loss: 0.4441 - acc: 0.7926 - val_loss: 0.4554 - val_acc: 0.7875\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.78575 to 0.78755, saving model to /home/erica/sean_go/2018-spring-hw0/multiple_lstm_without_tfidf_classifier.h5\n",
      "Epoch 6/200\n",
      "180000/180000 [==============================] - 778s 4ms/step - loss: 0.4344 - acc: 0.7984 - val_loss: 0.4595 - val_acc: 0.7861\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.78755\n",
      "Epoch 7/200\n",
      "180000/180000 [==============================] - 781s 4ms/step - loss: 0.4248 - acc: 0.8039 - val_loss: 0.4591 - val_acc: 0.7873\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.78755\n",
      "Epoch 8/200\n",
      "180000/180000 [==============================] - 836s 5ms/step - loss: 0.4156 - acc: 0.8085 - val_loss: 0.4577 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.78755\n",
      "Epoch 9/200\n",
      "180000/180000 [==============================] - 834s 5ms/step - loss: 0.4077 - acc: 0.8133 - val_loss: 0.4605 - val_acc: 0.7839\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.78755\n",
      "Epoch 10/200\n",
      "180000/180000 [==============================] - 837s 5ms/step - loss: 0.4011 - acc: 0.8169 - val_loss: 0.4711 - val_acc: 0.7824\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.78755\n",
      "Epoch 11/200\n",
      "180000/180000 [==============================] - 835s 5ms/step - loss: 0.3933 - acc: 0.8203 - val_loss: 0.4714 - val_acc: 0.7834\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.78755\n",
      "Epoch 12/200\n",
      "180000/180000 [==============================] - 833s 5ms/step - loss: 0.3868 - acc: 0.8246 - val_loss: 0.4651 - val_acc: 0.7826\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.78755\n",
      "Epoch 13/200\n",
      "180000/180000 [==============================] - 830s 5ms/step - loss: 0.3808 - acc: 0.8274 - val_loss: 0.4775 - val_acc: 0.7808\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.78755\n",
      "Epoch 14/200\n",
      "180000/180000 [==============================] - 829s 5ms/step - loss: 0.3751 - acc: 0.8301 - val_loss: 0.4839 - val_acc: 0.7795\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.78755\n",
      "Epoch 15/200\n",
      "180000/180000 [==============================] - 828s 5ms/step - loss: 0.3701 - acc: 0.8328 - val_loss: 0.4831 - val_acc: 0.7771\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.78755\n",
      "Epoch 16/200\n",
      "180000/180000 [==============================] - 829s 5ms/step - loss: 0.3655 - acc: 0.8348 - val_loss: 0.4862 - val_acc: 0.7776\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.78755\n",
      "Epoch 17/200\n",
      "180000/180000 [==============================] - 831s 5ms/step - loss: 0.3604 - acc: 0.8375 - val_loss: 0.4951 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.78755\n",
      "Epoch 18/200\n",
      "180000/180000 [==============================] - 831s 5ms/step - loss: 0.3568 - acc: 0.8388 - val_loss: 0.4907 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.78755\n",
      "Epoch 19/200\n",
      "180000/180000 [==============================] - 830s 5ms/step - loss: 0.3523 - acc: 0.8419 - val_loss: 0.5171 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.78755\n",
      "Epoch 20/200\n",
      "180000/180000 [==============================] - 830s 5ms/step - loss: 0.3483 - acc: 0.8441 - val_loss: 0.5036 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.78755\n",
      "Epoch 21/200\n",
      "180000/180000 [==============================] - 832s 5ms/step - loss: 0.3454 - acc: 0.8452 - val_loss: 0.5184 - val_acc: 0.7752\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.78755\n",
      "Epoch 22/200\n",
      "180000/180000 [==============================] - 831s 5ms/step - loss: 0.3428 - acc: 0.8456 - val_loss: 0.5127 - val_acc: 0.7721\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.78755\n",
      "Epoch 23/200\n",
      " 52800/180000 [=======>......................] - ETA: 9:28 - loss: 0.3314 - acc: 0.8513"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(training_vector_x.shape[1], training_vector_x.shape[2]),\n",
    "               dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(48, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(48, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(48, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(16, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model_name = \"multiple_lstm_without_tfidf_classifier.h5\"\n",
    "model_path = os.path.join(os.getcwd(), model_name)\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_acc', save_best_only=True, verbose=1)\n",
    "earlystop = EarlyStopping(monitor='val_acc', patience=20, verbose=1)\n",
    "\n",
    "model_history = model.fit(training_vector_x, training_y, validation_data=(validation_vector_x, validation_y), \n",
    "                          epochs=200, batch_size=50,\n",
    "                          callbacks = [checkpoint, earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_acc = model_history.history['acc']\n",
    "val_acc = model_history.history['val_acc']\n",
    "\n",
    "plt.plot(training_acc, label=\"training_accuracy\")\n",
    "plt.plot(val_acc, label=\"validation_accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary Accuracy\")\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "\n",
    "# model_path = os.path.join(os.getcwd(), model_name)\n",
    "# model  = load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(testing_vector_x)\n",
    "print(result.shape)\n",
    "print(result)\n",
    "result = (result>0.5).astype(int)\n",
    "result = result.reshape(-1)\n",
    "print(result.shape)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_frame = pd.DataFrame({\n",
    "    'id': testing_dataframe['Id'].tolist(),\n",
    "    'label': result })\n",
    "print(result_frame.head())\n",
    "output_path = os.path.join(os.getcwd(), \"result\", \"multiple_lstm_without_tfidf.csv\")\n",
    "result_frame.to_csv(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
